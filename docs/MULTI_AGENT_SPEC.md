# ğŸ¯ ××©×™××” ×œ-Claude Code: ×”×•×¡×¤×ª Multi-Agent Ensemble

## ××˜×¨×”
×œ×”×•×¡×™×£ ×œ×¤×¨×•×™×§×˜ Architect Agent ××¢×¨×›×ª Multi-Agent ×©×‘×” 4 ×¡×•×›× ×™× (×›×œ ××—×“ ××•×“×œ ××—×¨) ×¢×•×‘×“×™× ×™×—×“:
- **Generator** (Claude) - ××¦×™×¢ ××¨×›×™×˜×§×˜×•×¨×”
- **Critic** (GPT) - ××‘×§×¨ ×•××•×¦× ×—×•×¨×™×
- **Cost/Ops** (Gemini) - ×‘×•×“×§ ×¢×œ×•×ª ×•×”×™×ª×›× ×•×ª
- **Synthesizer** (Claude) - ×××–×’ ×œ×ª×•×¦×¨ ×¡×•×¤×™

---

## ××‘× ×” ×”×¨×™×¤×• ×”× ×•×›×—×™

```
architect-agent/
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ config.py                    # Settings - ×›×‘×¨ ×™×© ANTHROPIC_API_KEY
â”‚   â”œâ”€â”€ agent/
â”‚   â”‚   â”œâ”€â”€ state.py                 # ProjectContext + ×›×œ ×”××•×“×œ×™×
â”‚   â”‚   â”œâ”€â”€ graph.py                 # LangGraph workflow + router node
â”‚   â”‚   â””â”€â”€ nodes/                   # 8 × ×•×“×™× ×§×™×™××™×
â”‚   â”‚       â”œâ”€â”€ intake.py
â”‚   â”‚       â”œâ”€â”€ priority.py
â”‚   â”‚       â”œâ”€â”€ conflict.py
â”‚   â”‚       â”œâ”€â”€ deep_dive.py
â”‚   â”‚       â”œâ”€â”€ pattern.py           # â† ×œ×”×—×œ×™×£ ×‘×œ×•×’×™×§×ª Multi-Agent
â”‚   â”‚       â”œâ”€â”€ feasibility.py       # â† ×œ×”×—×œ×™×£ ×‘×œ×•×’×™×§×ª Multi-Agent
â”‚   â”‚       â”œâ”€â”€ blueprint.py         # â† ×œ×”×—×œ×™×£ ×‘×œ×•×’×™×§×ª Multi-Agent
â”‚   â”‚       â””â”€â”€ critic.py            # â† ×œ×”×—×œ×™×£ ×‘×œ×•×’×™×§×ª Multi-Agent
â”‚   â”œâ”€â”€ knowledge/
â”‚   â”‚   â”œâ”€â”€ patterns.py              # 6 patterns ×¢× metadata
â”‚   â”‚   â””â”€â”€ decision_matrix.py       # ××¢×¨×›×ª × ×™×§×•×“ ×“×˜×¨××™× ×™×¡×˜×™×ª
â”‚   â”œâ”€â”€ llm/
â”‚   â”‚   â”œâ”€â”€ client.py                # Claude client - ×¦×¨×™×š ×œ×”×¤×©×™×˜ ×œ-Protocol
â”‚   â”‚   â””â”€â”€ prompts.py               # ×¤×¨×•××¤×˜×™× - ×¦×¨×™×š ×œ×”×•×¡×™×£
â”‚   â”œâ”€â”€ db/                          # MongoDB - ×§×™×™× ×•×¢×•×‘×“
â”‚   â””â”€â”€ api/                         # FastAPI - ×§×™×™× ×•×¢×•×‘×“
â”œâ”€â”€ requirements.txt
â””â”€â”€ .env.example
```

---

## ××” ×›×‘×¨ ×§×™×™× ×‘×§×•×“ (×œ× ×œ×©×›×¤×œ!)

### ×‘-state.py - ×©×“×•×ª tracking ×›×‘×¨ ×§×™×™××™×:
```python
# ×©×“×•×ª ×©×›×‘×¨ ×§×™×™××™× - ×œ× ×œ×”×•×¡×™×£ ×©×•×‘!
revision_count: int = 0
last_pattern: Optional[str] = None
last_confidence_reason: Optional[str] = None
waiting_for_user: bool = False
```

### ×‘-graph.py - Router node ×›×‘×¨ ×§×™×™×:
```python
# Router ×©××—×œ×™×˜ ×××™×¤×” ×œ×”×ª×—×™×œ ×‘×”×ª×× ×œ-state:
# - ×× ×™×© blueprint â†’ ×××©×™×š ×-deep_dive
# - ×× ×™×© proposed_architecture â†’ ×××©×™×š ×-assess_feasibility
# - ×× ×™×© requirements â†’ ×××©×™×š ×-priority
# - ××—×¨×ª â†’ ××ª×—×™×œ ×-intake
```

### ×‘-critic.py - ×œ×•×’×™×§×ª loop prevention ×›×‘×¨ ×§×™×™××ª:
```python
# 5 ×›×œ×œ×™× ×œ×× ×™×¢×ª ×œ×•×¤×™×:
# 1. missing_info â†’ waiting_for_user=True, ×œ× ×—×•×–×¨×™× ××—×•×¨×”
# 2. confidence >= 0.5 â†’ ×™×•×¦××™× ×¢× assumptions
# 3. revision_count >= 2 â†’ ××¡×™×™××™×
# 4. same pattern â†’ ×œ× ×—×•×–×¨×™× ×©×•×‘
# 5. max_iterations â†’ ×™×¦×™××”
```

---

## ××” ×¦×¨×™×š ×œ×××©

### 1. ×™×¦×™×¨×ª src/llm/base.py - Protocol ×‘×¡×™×¡×™ (×—×“×©!)

```python
"""
×××©×§ ××—×™×“ ×œ×›×œ ×¡×¤×§×™ ×”-LLM.
×›×œ ×”-nodes ×™×©×ª××©×• ×‘-BaseLLMClient ×‘××§×•× LLMClient.
"""
from abc import ABC, abstractmethod
from typing import TypeVar, Type, Optional

T = TypeVar('T')

class BaseLLMClient(ABC):
    """Protocol ××—×™×“ ×œ×›×œ ×¡×¤×§×™ LLM."""

    @abstractmethod
    async def generate(
        self,
        prompt: str,
        system_prompt: Optional[str] = None,
        max_tokens: int = 4096,
        temperature: float = 0.7
    ) -> str:
        """×™×¦×™×¨×ª ×˜×§×¡×˜ ×—×•×¤×©×™."""
        ...

    @abstractmethod
    async def generate_structured(
        self,
        prompt: str,
        response_model: Type[T],
        system_prompt: Optional[str] = None,
        max_tokens: int = 4096,
        temperature: float = 0.7
    ) -> T:
        """×™×¦×™×¨×ª ×¤×œ×˜ ××•×‘× ×” ×œ×¤×™ Pydantic model."""
        ...

    @abstractmethod
    async def generate_with_history(
        self,
        messages: list,
        system_prompt: Optional[str] = None,
        max_tokens: int = 4096,
        temperature: float = 0.7
    ) -> str:
        """×™×¦×™×¨×ª ×˜×§×¡×˜ ×¢× ×”×™×¡×˜×•×¨×™×™×ª ×©×™×—×”."""
        ...
```

### 2. ×”×¨×—×‘×ª config.py - ×”×•×¡×¤×ª API keys

```python
# ×œ×”×•×¡×™×£ ×œ-Settings:
OPENAI_API_KEY: str = ""           # ×œ-Critic (GPT)
GOOGLE_API_KEY: str = ""           # ×œ-Cost/Ops (Gemini)

# ××•×“×œ×™× ×¡×¤×¦×™×¤×™×™× ×œ×›×œ ×¡×•×›×Ÿ  ### ×“×¨×•×© ×¢×“×›×•×Ÿ ××•×“×œ×™× ×¢×“×›× ×™ ×™×•×ª×¨
GENERATOR_MODEL: str = "claude-sonnet-4-5-20250929"     # Claude
CRITIC_MODEL: str = "gpt-4o"                          # OpenAI
COST_OPS_MODEL: str = "gemini-pro"               # Google
SYNTHESIZER_MODEL: str = "claude-sonnet-4-5-20250929"  # Claude
```

### 3. ×™×¦×™×¨×ª src/llm/multi_provider.py - Client ×œ×›×œ ×”×¡×¤×§×™×

```python
"""
Client ××—×™×“ ×©×ª×•××š ×‘-3 ×¡×¤×§×™× ×•××××© ××ª BaseLLMClient.
"""
from typing import Literal, Type, TypeVar, Optional
import logging

from anthropic import AsyncAnthropic
from openai import AsyncOpenAI
import google.generativeai as genai

from .base import BaseLLMClient
from ..config import settings

logger = logging.getLogger(__name__)
T = TypeVar('T')


class MultiProviderLLM(BaseLLMClient):
    """Unified interface for multiple LLM providers."""

    def __init__(self):
        self._anthropic = AsyncAnthropic(api_key=settings.ANTHROPIC_API_KEY)
        self._openai = AsyncOpenAI(api_key=settings.OPENAI_API_KEY) if settings.OPENAI_API_KEY else None
        self._gemini_configured = False
        if settings.GOOGLE_API_KEY:
            genai.configure(api_key=settings.GOOGLE_API_KEY)
            self._gemini_configured = True

    async def call(
        self,
        provider: Literal["anthropic", "openai", "google"],
        model: str,
        system_prompt: str,
        user_prompt: str,
        response_model: Optional[Type[T]] = None
    ) -> dict | T:
        """
        ×§×¨×™××” ××—×™×“×” ×œ×›×œ ×¡×¤×§.

        Args:
            provider: ×”×¡×¤×§ ×œ×©×™××•×©
            model: ×©× ×”××•×“×œ
            system_prompt: ×”×•×¨××•×ª ××¢×¨×›×ª
            user_prompt: ×”×¤×¨×•××¤×˜ ×©×œ ×”××©×ª××©
            response_model: Pydantic model ×œ×¤×œ×˜ ××•×‘× ×” (××•×¤×¦×™×•× ×œ×™)

        Returns:
            dict ××• Pydantic model
        """
        try:
            if provider == "anthropic":
                return await self._call_anthropic(model, system_prompt, user_prompt, response_model)
            elif provider == "openai":
                return await self._call_openai(model, system_prompt, user_prompt, response_model)
            elif provider == "google":
                return await self._call_google(model, system_prompt, user_prompt, response_model)
            else:
                raise ValueError(f"Unknown provider: {provider}")
        except Exception as e:
            # ×× Anthropic × ×›×©×œ - ××™×Ÿ fallback, ××¢×œ×™× ××ª ×”×©×’×™××”
            if provider == "anthropic":
                logger.error(f"Anthropic failed with no fallback: {e}")
                raise
            # ××—×¨×ª - fallback ×œ-Claude
            logger.warning(f"{provider} failed: {e}, falling back to Claude")
            return await self._call_anthropic(
                settings.GENERATOR_MODEL,
                system_prompt,
                user_prompt,
                response_model
            )

    async def _call_anthropic(self, model: str, system: str, prompt: str, response_model: Optional[Type[T]]) -> dict | T:
        """×§×¨×™××” ×œ-Claude."""
        # ××™××•×© ×“×•××” ×œ-client.py ×”×§×™×™×
        ...

    async def _call_openai(self, model: str, system: str, prompt: str, response_model: Optional[Type[T]]) -> dict | T:
        """×§×¨×™××” ×œ-GPT."""
        if not self._openai:
            raise RuntimeError("OpenAI not configured")

        messages = [
            {"role": "system", "content": system},
            {"role": "user", "content": prompt}
        ]

        if response_model:
            # Structured output ×¢× response_format
            response = await self._openai.beta.chat.completions.parse(
                model=model,
                messages=messages,
                response_format=response_model
            )
            return response.choices[0].message.parsed
        else:
            response = await self._openai.chat.completions.create(
                model=model,
                messages=messages
            )
            return {"content": response.choices[0].message.content}

    async def _call_google(self, model: str, system: str, prompt: str, response_model: Optional[Type[T]]) -> dict | T:
        """×§×¨×™××” ×œ-Gemini."""
        if not self._gemini_configured:
            raise RuntimeError("Gemini not configured")

        gemini_model = genai.GenerativeModel(
            model_name=model,
            system_instruction=system
        )

        response = await gemini_model.generate_content_async(prompt)

        if response_model:
            # ×¤×¨×¡×•×¨ JSON ×œ×ª×•×š Pydantic
            import json
            data = json.loads(response.text)
            return response_model(**data)
        else:
            return {"content": response.text}

    # ××™××•×© ×××©×§ BaseLLMClient
    async def generate(self, prompt: str, system_prompt: Optional[str] = None, **kwargs) -> str:
        result = await self.call("anthropic", settings.GENERATOR_MODEL, system_prompt or "", prompt)
        return result.get("content", "") if isinstance(result, dict) else str(result)

    async def generate_structured(self, prompt: str, response_model: Type[T], system_prompt: Optional[str] = None, **kwargs) -> T:
        return await self.call("anthropic", settings.GENERATOR_MODEL, system_prompt or "", prompt, response_model)

    async def generate_with_history(self, messages: list, system_prompt: Optional[str] = None, **kwargs) -> str:
        # ××™××•×© ×¢× ×”×™×¡×˜×•×¨×™×”
        ...
```

### 4. ×™×¦×™×¨×ª src/agent/nodes/experts/ - ×ª×™×§×™×™×” ×—×“×©×” ×œ×¡×•×›× ×™×

```
src/agent/nodes/experts/
â”œâ”€â”€ __init__.py
â”œâ”€â”€ generator.py      # Agent 1 - Solution Architect (Claude)
â”œâ”€â”€ critic.py         # Agent 2 - Red Team (GPT)
â”œâ”€â”€ cost_ops.py       # Agent 3 - Feasibility (Gemini)
â”œâ”€â”€ synthesizer.py    # Agent 4 - Blueprint Editor (Claude)
â””â”€â”€ schemas.py        # Pydantic schemas ×œ×¤×œ×˜ ××—×™×“
```

### 5. schemas.py - ×¤×•×¨××˜ ×¤×œ×˜ ××—×™×“ ×œ×›×œ ×¡×•×›×Ÿ

```python
"""
×¡×›×™××•×ª Pydantic ×œ×¤×œ×˜ ××—×™×“ ××›×œ ×”×¡×•×›× ×™×.
"""
from typing import List, Optional, Literal
from pydantic import BaseModel, Field


class KeyDecision(BaseModel):
    """×”×—×œ×˜×” ××¨×›×™×˜×§×˜×•× ×™×ª."""
    title: str
    decision: str
    rationale: str
    alternatives_considered: List[str] = []


class TechComponent(BaseModel):
    """×¨×›×™×‘ ×˜×›× ×•×œ×•×’×™ ×‘-stack."""
    name: str
    role: str  # ××” ×”×•× ×¢×•×©×”
    justification: str  # ×œ××” ×‘×—×¨× ×• ×‘×•


class Risk(BaseModel):
    """×¡×™×›×•×Ÿ ××–×•×”×”."""
    description: str
    severity: Literal["low", "medium", "high", "critical"]
    mitigation: str
    owner: Optional[str] = None  # ××™ ××—×¨××™ ×œ×˜×¤×œ


class Unknown(BaseModel):
    """××™×“×¢ ×—×¡×¨."""
    question: str
    impact: Literal["low", "medium", "high"]
    default_assumption: Optional[str] = None  # ××” × × ×™×— ×× ×œ× × ×§×‘×œ ×ª×©×•×‘×”


class Issue(BaseModel):
    """×‘×¢×™×” ×©××¦× ×”-Critic."""
    description: str
    severity: Literal["minor", "major", "critical"]
    location: str  # ××™×¤×” ×‘×”×¦×¢×”
    suggested_fix: str


class Fix(BaseModel):
    """×ª×™×§×•×Ÿ ××•×¦×¢."""
    issue: str
    fix: str
    effort: Literal["trivial", "small", "medium", "large"]


class ExpertOutput(BaseModel):
    """×¤×œ×˜ ××—×™×“ ××›×œ ×¡×•×›×Ÿ."""
    summary: str = Field(..., description="×¡×™×›×•× ×‘-2-3 ××©×¤×˜×™×")
    pattern_recommendation: str = Field(..., description="Pattern ××•××œ×¥")
    key_decisions: List[KeyDecision] = Field(default_factory=list, min_length=3, max_length=7)
    tech_stack: List[TechComponent] = Field(default_factory=list)
    risks: List[Risk] = Field(default_factory=list)
    unknowns: List[Unknown] = Field(default_factory=list)
    mermaid_diagram: Optional[str] = None
    cost_band: Literal["low", "medium", "high"] = "medium"
    ops_band: Literal["low", "medium", "high"] = "medium"
    confidence: float = Field(..., ge=0, le=1, description="×¨××ª ×‘×™×˜×—×•×Ÿ 0-1")


class CriticOutput(ExpertOutput):
    """×¤×œ×˜ ×¡×¤×¦×™×¤×™ ×œ-Critic."""
    issues_found: List[Issue] = Field(default_factory=list)
    suggested_fixes: List[Fix] = Field(default_factory=list)
    questions_for_user: List[str] = Field(default_factory=list, description="×©××œ×•×ª ×©×—×™×™×‘ ×œ×©××•×œ")
    failure_modes: List[str] = Field(default_factory=list, max_length=5, description="Top 5 failure modes")
    low_confidence_reason: Optional[Literal[
        "missing_info",
        "conflicting_constraints",
        "weak_justification",
        "wrong_pattern",
        "risks_not_mitigated",
        "other"
    ]] = None


class CostOpsOutput(BaseModel):
    """×¤×œ×˜ ×¡×¤×¦×™×¤×™ ×œ-Cost/Ops."""
    cost_band: Literal["low", "medium", "high"]
    cost_justification: str
    ops_band: Literal["low", "medium", "high"]
    ops_justification: str
    top_cost_drivers: List[str] = Field(default_factory=list, min_length=3, max_length=7)
    top_ops_pains: List[str] = Field(default_factory=list, min_length=3, max_length=7)
    cheaper_alternatives: List[dict] = Field(default_factory=list, description="[{current, alternative, savings}]")
    risk_reducers: List[str] = Field(default_factory=list, min_length=3, max_length=5)
    team_fit_score: float = Field(..., ge=0, le=1)
    team_fit_issues: List[str] = Field(default_factory=list)


class SynthesizerOutput(BaseModel):
    """×¤×œ×˜ ×¡×•×¤×™ ××”-Synthesizer."""
    executive_summary: str = Field(..., description="×¡×™×›×•× ×× ×”×œ×™× ×‘-4-8 ×©×•×¨×•×ª ×‘×¢×‘×¨×™×ª")
    final_pattern: str
    final_tech_stack: List[TechComponent]
    final_decisions: List[KeyDecision]
    mermaid_diagram: str
    roadmap: dict = Field(..., description="{phase1: [tasks], phase2: [tasks], ...}")
    adrs: List[dict] = Field(default_factory=list, min_length=3, max_length=6)
    assumptions: List[str] = Field(default_factory=list, min_length=3, max_length=8)
    open_unknowns: List[Unknown] = Field(default_factory=list)
    final_risks: List[Risk] = Field(default_factory=list)
    confidence: float = Field(..., ge=0, le=1)
    dissenting_opinions: List[str] = Field(default_factory=list, description="×“×¢×•×ª ××™×¢×•×˜ ××”×¡×•×›× ×™× ×”××—×¨×™×")
```

### 6. ×¢×“×›×•×Ÿ state.py - ×”×•×¡×¤×ª ×©×“×•×ª expert outputs ×‘×œ×‘×“

```python
# ×œ×”×•×¡×™×£ ×œ-ProjectContext (×”×©××¨ ×›×‘×¨ ×§×™×™×!):

from .nodes.experts.schemas import ExpertOutput, CriticOutput, CostOpsOutput, SynthesizerOutput

class ProjectContext(BaseModel):
    # ... ×©×“×•×ª ×§×™×™××™× ...

    # ---- Multi-Agent Outputs ----
    generator_output: Optional[ExpertOutput] = None
    critic_output: Optional[CriticOutput] = None
    cost_ops_output: Optional[CostOpsOutput] = None
    synthesizer_output: Optional[SynthesizerOutput] = None

    # ---- Change Log (×œ×“×™×‘×•×’) ----
    change_log: List[dict] = Field(default_factory=list)

    def log_change(self, agent: str, change: str):
        """×¨×™×©×•× ×©×™× ×•×™ ×œ×“×™×‘×•×’."""
        self.change_log.append({
            "agent": agent,
            "change": change,
            "iteration": self.iteration_count
        })
```

### 7. ×¢×“×›×•×Ÿ graph.py - ×–×¨×™××” ×—×“×©×”

```python
"""
×”×–×¨×™××” ×”×—×“×©×” ×¢× Multi-Agent:

router â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â†“ (new session)                       â”‚ (returning user)
intake â†’ priority â†’ conflict â†’ deep_dive â†â”˜
                                    â†“
                              generator
                                    â†“
                         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                         â†“                     â†“
                      critic              cost_ops
                         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                    â†“
                              synthesizer
                                    â†“
                              final_gate
                                    â†“
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â†“               â†“               â†“
                   END          ask_user        generator
                                    â†“               â†‘
                                router â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
"""

from langgraph.graph import StateGraph, END
from typing import Dict, Any

from .state import ProjectContext
from .nodes import intake_node, priority_node, conflict_node, deep_dive_node
from .nodes.experts import generator_node, critic_node, cost_ops_node, synthesizer_node
from ..llm.multi_provider import MultiProviderLLM


def create_architect_graph(llm_client: MultiProviderLLM = None):
    """×™×¦×™×¨×ª ×”×’×¨×£ ×¢× ×ª××™×›×” ×‘××•×œ×˜×™-××™×™×’'× ×˜."""

    if llm_client is None:
        llm_client = MultiProviderLLM()

    graph = StateGraph(ProjectContext)

    # ========================================
    # PHASE 1: Information Gathering (×§×™×™×)
    # ========================================

    async def _intake(state: ProjectContext) -> Dict[str, Any]:
        ctx, reply = await intake_node(state, llm_client)
        return ctx.model_dump()

    async def _priority(state: ProjectContext) -> Dict[str, Any]:
        ctx, reply = await priority_node(state, llm_client)
        return ctx.model_dump()

    async def _conflict(state: ProjectContext) -> Dict[str, Any]:
        ctx, reply = await conflict_node(state, llm_client)
        return ctx.model_dump()

    async def _deep_dive(state: ProjectContext) -> Dict[str, Any]:
        ctx, reply = await deep_dive_node(state, llm_client)
        return ctx.model_dump()

    # ========================================
    # PHASE 2: Expert Panel (×—×“×©)
    # ========================================

    async def _generator(state: ProjectContext) -> Dict[str, Any]:
        # ××™×¤×•×¡ waiting_for_user - ×× ×”×’×¢× ×• ×œ×›××Ÿ, ×”××©×ª××© ×›×‘×¨ ×”×’×™×‘
        state.waiting_for_user = False
        ctx, reply = await generator_node(state, llm_client)
        return ctx.model_dump()

    async def _critic(state: ProjectContext) -> Dict[str, Any]:
        ctx, reply = await critic_node(state, llm_client)
        return ctx.model_dump()

    async def _cost_ops(state: ProjectContext) -> Dict[str, Any]:
        ctx, reply = await cost_ops_node(state, llm_client)
        return ctx.model_dump()

    # ========================================
    # PHASE 3: Synthesis (×—×“×©)
    # ========================================

    async def _synthesizer(state: ProjectContext) -> Dict[str, Any]:
        ctx, reply = await synthesizer_node(state, llm_client)
        return ctx.model_dump()

    async def _final_gate(state: ProjectContext) -> Dict[str, Any]:
        ctx, reply, next_node = await final_gate_node(state)
        ctx_dict = ctx.model_dump()
        ctx_dict["_routing_hint"] = next_node
        return ctx_dict

    # ========================================
    # ROUTER NODE (×§×™×™× - ×œ× ×œ×©× ×•×ª!)
    # ========================================

    def _route_entry(state) -> str:
        """Router ×©××—×œ×™×˜ ×××™×¤×” ×œ×”×ª×—×™×œ/×œ×”××©×™×š."""
        if isinstance(state, dict):
            state_dict = state
        else:
            state_dict = state.model_dump()

        # ×× ×™×© synthesizer_output ×•×—×™×›×™× ×• ×œ××©×ª××© - ×—×•×–×¨×™× ×-ask_user
        # ×”×¢×¨×”: waiting_for_user ×™××•×¤×¡ ×‘-generator node (×¨××” ×œ××˜×”)
        if state_dict.get("synthesizer_output") and state_dict.get("waiting_for_user"):
            return "generator"

        # ×× ×™×© generator_output - ×××©×™×›×™× ×-critic
        if state_dict.get("generator_output"):
            return "critic"

        # ×× ×™×© requirements - ×××©×™×›×™× ×-priority
        if state_dict.get("requirements"):
            return "generator"  # ×“×™×œ×•×’ ×¢×œ intake ×× ×™×© ×›×‘×¨ requirements

        return "intake"

    # ========================================
    # ADD NODES
    # ========================================

    # Router
    graph.add_node("router", lambda state: state)

    # Phase 1
    graph.add_node("intake", _intake)
    graph.add_node("priority", _priority)
    graph.add_node("conflict", _conflict)
    graph.add_node("deep_dive", _deep_dive)

    # Phase 2
    graph.add_node("generator", _generator)
    graph.add_node("critic", _critic)
    graph.add_node("cost_ops", _cost_ops)

    # Phase 3
    graph.add_node("synthesizer", _synthesizer)
    graph.add_node("final_gate", _final_gate)

    # ========================================
    # SET ENTRY POINT
    # ========================================

    graph.set_entry_point("router")

    # ========================================
    # ADD EDGES
    # ========================================

    # Router edges
    graph.add_conditional_edges(
        "router",
        _route_entry,
        {
            "intake": "intake",
            "generator": "generator",
            "critic": "critic",
        }
    )

    # Phase 1 edges (linear)
    graph.add_edge("intake", "priority")
    graph.add_edge("priority", "conflict")
    graph.add_edge("conflict", "deep_dive")
    graph.add_edge("deep_dive", "generator")

    # Phase 2 edges (generator â†’ parallel critics)
    # ×”×¢×¨×”: LangGraph ×ª×•××š ×‘-parallel execution ×¢× fan-out
    graph.add_edge("generator", "critic")
    graph.add_edge("generator", "cost_ops")

    # After critics â†’ synthesizer
    graph.add_edge("critic", "synthesizer")
    graph.add_edge("cost_ops", "synthesizer")

    # Synthesizer â†’ final gate
    graph.add_edge("synthesizer", "final_gate")

    # Final gate routing
    def _route_from_gate(state) -> str:
        if isinstance(state, dict):
            hint = state.get("_routing_hint")
        else:
            hint = getattr(state, "_routing_hint", None)

        if hint == "ask_user":
            return "end"  # ×™×•×¦××™× ×•××—×›×™× ×œ×ª×©×•×‘×”
        elif hint == "generator":
            return "generator"
        else:
            return "end"

    graph.add_conditional_edges(
        "final_gate",
        _route_from_gate,
        {
            "end": END,
            "generator": "generator",
        }
    )

    return graph.compile()
```

---

## ×¤×¨×•××¤×˜×™× ×œ×›×œ ×¡×•×›×Ÿ

### Generator (Claude) - src/llm/prompts.py

```python
GENERATOR_SYSTEM_PROMPT = """
××ª×” Solution Architect ×‘×›×™×¨. ×ª×¤×§×™×“×š:
1. ×œ×§×‘×œ ProjectContext ×¢× ×“×¨×™×©×•×ª, ××™×œ×•×¦×™×, ×•×¡×“×¨×™ ×¢×“×™×¤×•×™×•×ª
2. ×œ×”×¦×™×¢ ××¨×›×™×˜×§×˜×•×¨×” ××œ××”: Pattern + Stack + Diagram + Roadmap + ADRs

×›×œ×œ×™×:
- ×ª×”×™×” ×¤×¨×§×˜×™: ×”×™×× ×¢ ××˜×›× ×•×œ×•×’×™×•×ª ×›×‘×“×•×ª ×× ××™×Ÿ ×”×¦×“×§×”
- ××œ ×ª×•×¡×™×£ ×“×¨×™×©×•×ª ×—×“×©×•×ª - ×× ×™×© ×”× ×—×”, ×¡××Ÿ ××•×ª×”
- ×”×ª×—×©×‘ ×‘-priorities: ×× velocity ×—×©×•×‘ ×™×•×ª×¨ ×-scalability, ××œ ×ª×¦×™×¢ Kubernetes
- ×”×—×–×¨ JSON ×‘×œ×‘×“ ×œ×¤×™ ×”×¡×›×™××” ExpertOutput
"""

GENERATOR_USER_PROMPT = """
ProjectContext:
{project_context_json}

Pattern Scores (××”××¢×¨×›×ª ×”×“×˜×¨××™× ×™×¡×˜×™×ª):
{scoring_results_json}

××©×™××”:
1. ×‘×—×¨ Pattern ××ª×•×š ×”-shortlist (×× ×•××§)
2. ×”×¦×¢ Tech Stack ××œ× (5-15 ×¨×›×™×‘×™×)
3. ×¦×•×¨ Mermaid diagram
4. ×ª×Ÿ Roadmap ×‘-3 ×¤××–×•×ª
5. ×¦×™×™×Ÿ 3-6 ×¡×™×›×•× ×™× + mitigation
6. ×¦×™×™×Ÿ Unknowns ×¨×§ ×× ×”×Ÿ Impact ×’×‘×•×”

×”×—×–×¨ JSON ×œ×¤×™ ×¡×›×™××ª ExpertOutput.
"""
```

### Critic (GPT)

```python
CRITIC_SYSTEM_PROMPT = """
××ª×” Red Team Architect. ××ª×” ×œ× ××ª×›× ×Ÿ ×××¤×¡.
××ª×” ×ª×•×§×£ ××ª ×”×”×¦×¢×” ×”×§×™×™××ª: ××•×¦× ×›×©×œ×™×, ×¡×™×›×•× ×™×, ×—×•×¡×¨×™×.
×”××˜×¨×”: ×œ×”×¤×•×š ××ª ×”×”×¦×¢×” ×œ×‘×˜×•×—×” ×™×•×ª×¨, ×™×©×™××” ×™×•×ª×¨, ×•×‘×¨×•×¨×” ×™×•×ª×¨.

×›×œ×œ×™×:
- ××œ ×ª×¦×™×¢ ××¨×›×™×˜×§×˜×•×¨×” ×—×“×©×” - ×¨×§ ×ª×§×Ÿ ××ª ×”×§×™×™××ª
- ××œ ×ª×•×¡×™×£ Tech Stack ×—×“×© ×‘×œ×™ ×”×¦×“×§×”
- ××§×“ ××ª ×”×‘×™×§×•×¨×ª ×‘×‘×¢×™×•×ª ×××™×ª×™×•×ª, ×œ× × ×™×˜×¤×™×§×™× ×’
"""

CRITIC_USER_PROMPT = """
ProjectContext:
{project_context_json}

Proposal ×©×œ Generator:
{generator_output_json}

××©×™××”:
1. ××¦× ×—×•×¨×™×/×›×©×œ×™×/×¡×ª×™×¨×•×ª (issues_found)
2. ×ª×Ÿ ×ª×™×§×•× ×™× ×××•×§×“×™× (suggested_fixes) - ×œ× "×ª×‘× ×” ××—×“×©"
3. ×¨×©×™××ª ×©××œ×•×ª ×©×—×™×™×‘ ×œ×©××•×œ ××ª ×”××©×ª××© (questions_for_user)
4. Top 5 Failure Modes
5. ×¦×™×™×Ÿ low_confidence_reason ×× ×™×© ×‘×¢×™×”:
   - missing_info: ×—×¡×¨ ××™×“×¢ ××”××©×ª××©
   - conflicting_constraints: ×¡×ª×™×¨×” ×‘×“×¨×™×©×•×ª
   - weak_justification: ×”×”×¦×¢×” ×œ× ×× ×•××§×ª ××¡×¤×™×§
   - wrong_pattern: ×¦×¨×™×š pattern ××—×¨
   - risks_not_mitigated: ×¡×™×›×•× ×™× ×œ× ××˜×•×¤×œ×™×

×”×—×–×¨ JSON ×œ×¤×™ ×¡×›×™××ª CriticOutput.
"""
```

### Cost/Ops (Gemini)

```python
COST_OPS_SYSTEM_PROMPT = """
××ª×” Cost & Ops Architect. ××ª×” ××¡×ª×›×œ ×¨×§ ×¢×œ:
- ×¢×œ×•×ª (cloud, licenses, development, maintenance)
- ××•×¨×›×‘×•×ª ×ª×¤×¢×•×œ (deployment, monitoring, debugging)
- ×™×¦×™×‘×•×ª (failure modes, recovery)
- ×™×›×•×œ×ª ×¦×•×•×ª (team skills, learning curve)

××ª×” ×œ× ××—×¤×© "×”×›×™ ×™×¤×”", ××œ× "×”×›×™ ×™×©×™×".
"""

COST_OPS_USER_PROMPT = """
ProjectContext:
{project_context_json}

Proposal ×©×œ Generator:
{generator_output_json}

Team Info:
- Size: {team_size}
- Experience: {team_experience}
- Current Stack: {current_stack}

××©×™××”:
1. ×ª×Ÿ cost_band (low/medium/high) + ×”×¡×‘×¨
2. ×ª×Ÿ ops_band (low/medium/high) + ×”×¡×‘×¨
3. top_cost_drivers (3-7 ×¤×¨×™×˜×™×)
4. top_ops_pains (3-7 × ×§×•×“×•×ª ×›××‘ ×ª×¤×¢×•×œ×™×•×ª)
5. cheaper_alternatives (×× ×™×©)
6. risk_reducers (3-5 ×“×¨×›×™× ×œ×”×§×˜×™×Ÿ ×¡×™×›×•×Ÿ)
7. team_fit_score + issues

×”×—×–×¨ JSON ×œ×¤×™ ×¡×›×™××ª CostOpsOutput.
"""
```

### Synthesizer (Claude)

```python
SYNTHESIZER_SYSTEM_PROMPT = """
××ª×” Blueprint Editor (×¢×•×¨×š ×¨××©×™).
××ª×” ××§×‘×œ ×”×¦×¢×” + ×‘×™×§×•×¨×ª + ×‘×“×™×§×ª ×¢×œ×•×ª, ×•××•×¦×™× Blueprint ××—×“ ×¡×•×¤×™.
××ª×” ××—×¨××™ ×¢×œ ×”×›×¨×¢×” ×›×©×™×© ××—×œ×•×§×ª.

×›×œ×œ ×”×›×¨×¢×”: ××™ ×©××ª××™× ×™×•×ª×¨ ×œ-priorities ×× ×¦×—.

×›×œ×œ×™×:
- ×”×‘×¡×™×¡ ×”×•× ×”×¦×¢×ª ×”-Generator - ×œ× ××ª×—×™×œ×™× ×××¤×¡
- ×©×œ×‘ ×ª×™×§×•× ×™× ××•×¦×“×§×™× ××”-Critic
- ×”×ª×—×©×‘ ×‘×¢×œ×•×ª/ops ×-Cost/Ops
- ×× ×™×© ××—×œ×•×§×ª - ×¦×™×™×Ÿ ××•×ª×” ×‘-dissenting_opinions
"""

SYNTHESIZER_USER_PROMPT = """
ProjectContext:
{project_context_json}

Priorities (×œ×¤×™ ×¡×“×¨ ×—×©×™×‘×•×ª):
{priorities_json}

Generator Output:
{generator_output_json}

Critic Output:
{critic_output_json}

Cost/Ops Output:
{cost_ops_output_json}

××©×™××”:
1. ×”×¤×§ Blueprint ×××•×—×“ - ×‘×¡×™×¡ ×-Generator + ×ª×™×§×•× ×™× ××•×¦×“×§×™×
2. ×‘×¡×ª×™×¨×” - ×ª×¢×“×™×£ ××” ×©××ª××™× ×œ-priorities
3. ×”×•×¡×£ Assumptions (3-8) ×•-open_unknowns
4. ×¢×“×›×Ÿ Mermaid diagram ×× × ×“×¨×©
5. ADRs ×¡×•×¤×™×™× (3-6)
6. executive_summary ×‘×¢×‘×¨×™×ª (4-8 ×©×•×¨×•×ª)
7. ×× ×™×© ×“×¢×•×ª ××™×¢×•×˜ ×©×œ× ×§×™×‘×œ×ª - ×¦×™×™×Ÿ ×‘-dissenting_opinions

×”×—×–×¨ JSON ×œ×¤×™ ×¡×›×™××ª SynthesizerOutput.
"""
```

---

## ×× ×™×¢×ª ×œ×•×¤×™× - ×—×•×§×™× ×§×¨×™×˜×™×™×

### ×‘-final_gate_node:

```python
async def final_gate_node(ctx: ProjectContext) -> Tuple[ProjectContext, str, Optional[str]]:
    """
    Gate ×©××—×œ×™×˜: ×œ×¡×™×™×, ×œ×©××•×œ, ××• ×œ×—×–×•×¨.

    ××©×ª××© ×‘×œ×•×’×™×§×” ×”×§×™×™××ª ×-critic_node (×œ× ×œ×©×›×¤×œ!).
    """

    synth = ctx.synthesizer_output
    critic = ctx.critic_output

    if not synth:
        return ctx, "××™×Ÿ ×¤×œ×˜ ××”-Synthesizer", None

    confidence = synth.confidence
    reason = critic.low_confidence_reason if critic else None
    current_pattern = synth.final_pattern

    # ×›×œ×œ 1: ××¡×¤×™×§ ×—×–×¨×•×ª - ×™×•×¦××™× ×¢× ×”×¡×ª×™×™×’×•×™×•×ª
    if ctx.revision_count >= 2:
        ctx.log_change("final_gate", "max revisions reached, exiting")
        return ctx, _build_max_revisions_reply(synth), None  # â†’ END

    # ×›×œ×œ 2: ××•×ª×• pattern - ×œ× ×—×•×–×¨×™× ×©×•×‘
    if current_pattern and current_pattern == ctx.last_pattern:
        ctx.log_change("final_gate", "pattern unchanged, exiting")
        return ctx, _build_no_improvement_reply(synth), None  # â†’ END

    # ×›×œ×œ 3: confidence ×’×‘×•×” - ×™×•×¦××™×
    if confidence >= 0.7:
        return ctx, _build_success_reply(synth), None  # â†’ END

    # ×›×œ×œ 4: confidence ×‘×™× ×•× ×™ - ×™×•×¦××™× ×¢× assumptions
    if confidence >= 0.5:
        return ctx, _build_with_assumptions_reply(synth), None  # â†’ END

    # ×›×œ×œ 5: ×—×¡×¨ ××™×“×¢ - ×©×•××œ×™× ××©×ª××© (×œ× ×—×•×–×¨×™× ×¤× ×™××™×ª!)
    if reason == "missing_info":
        questions = critic.questions_for_user if critic else []
        ctx.waiting_for_user = True
        return ctx, _build_questions_reply(questions), "ask_user"

    # ×›×œ×œ 6: ×‘×¢×™×” ××—×¨×ª - ×× ×¡×™× ×©×•×‘ ×¢× ×©×™× ×•×™
    ctx.revision_count += 1
    ctx.last_pattern = current_pattern
    ctx.log_change("final_gate", f"revision {ctx.revision_count}, trying again")
    return ctx, _build_retry_reply(synth, reason), "generator"
```

---

## API Keys ×‘-.env

```bash
# Anthropic (Claude) - Generator + Synthesizer
ANTHROPIC_API_KEY=sk-ant-...

# OpenAI (GPT) - Critic
OPENAI_API_KEY=sk-...

# Google (Gemini) - Cost/Ops
GOOGLE_API_KEY=AIza...

# Model overrides (optional)
GENERATOR_MODEL=claude-sonnet-4-20250514
CRITIC_MODEL=gpt-4o
COST_OPS_MODEL=gemini-1.5-pro
SYNTHESIZER_MODEL=claude-sonnet-4-20250514
```

---

## ×¡×“×¨ ×¢×‘×•×“×” ××•××œ×¥

1. **src/llm/base.py** - ×™×¦×™×¨×ª Protocol/ABC âœ¨ ×—×“×©!
2. **config.py** - ×”×•×¡×£ API keys ×—×“×©×™×
3. **src/llm/multi_provider.py** - ×¦×•×¨ client ××—×™×“ (××××© BaseLLMClient)
4. **×¢×“×›×Ÿ type hints** ×‘×›×œ nodes: `LLMClient` â†’ `BaseLLMClient`
5. **src/agent/nodes/experts/schemas.py** - Pydantic models
6. **src/agent/nodes/experts/*.py** - 4 ×”×¡×•×›× ×™×
7. **src/llm/prompts.py** - ×”×•×¡×£ ×¤×¨×•××¤×˜×™×
8. **state.py** - ×”×•×¡×£ ×¨×§ expert outputs (×”×©××¨ ×›×‘×¨ ×§×™×™×!)
9. **graph.py** - ×¢×“×›×Ÿ ××ª ×”×–×¨×™××”, ×©××•×¨ ×¢×œ router ×”×§×™×™×
10. **×‘×“×™×§×•×ª** - ×•×“× ×©×¢×•×‘×“ ×¢× fallback ×œ-Claude

---

## ×”×¢×¨×•×ª ×—×©×•×‘×•×ª

- **Parallel execution**: Generator ×¨×¥ ×§×•×“×, ××—×¨ ×›×š Critic + Cost/Ops ×‘××§×‘×™×œ (LangGraph fan-out)
- **Scoring × ×©××¨**: ××¢×¨×›×ª ×”× ×™×§×•×“ ×”×“×˜×¨××™× ×™×¡×˜×™×ª ×××©×™×›×” ×œ×ª×ª shortlist ×œ-Generator
- **Fallback**: ×× ×¡×¤×§ ××—×“ × ×•×¤×œ, MultiProviderLLM ×—×•×–×¨ ×œ-Claude ××•×˜×•××˜×™×ª
- **×”×–×¨×™××” ×”×§×™×™××ª**: intake â†’ priority â†’ conflict â†’ deep_dive × ×©××¨×™× ×›××• ×©×”×
- **Router ×§×™×™×**: ×œ× ×œ×’×¢×ª ×‘-router - ×”×•× ×›×‘×¨ ×ª×•××š ×‘×”××©×š ×× ×§×•×“×•×ª ×©×•× ×•×ª
- **Loop prevention ×§×™×™×**: ×”×œ×•×’×™×§×” ×‘-critic_node ×¢×•×‘×¨×ª ×œ-final_gate

---

## ××” ×œ× ×œ×©× ×•×ª

âš ï¸ **××–×”×¨×”**: ×”×§×•×“ ×”×‘× ×›×‘×¨ ×§×™×™× ×•×¢×•×‘×“ - ×œ× ×œ×©×›×¤×œ ××• ×œ×“×¨×•×¡:

- `state.py`: revision_count, last_pattern, waiting_for_user
- `graph.py`: _route_entry, router node
- `critic.py`: ×œ×•×’×™×§×ª loop prevention (×œ×”×¢×‘×™×¨ ×œ-final_gate, ×œ× ×œ×©×›×¤×œ)

---

## ×œ×§×¨×™××” × ×•×¡×¤×ª

- [××¡××š ×”×ª×™×›× ×•×Ÿ ×”×¨××©×•× ×™](https://github.com/amirbiron/architect-agent/blob/8f9b765e7a59986447f640d6bba32aa776521704/MultiAgent'sPlan.md)

×‘×”×¦×œ×—×”! ğŸš€
